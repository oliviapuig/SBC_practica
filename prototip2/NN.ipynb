{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproces_books import preprocess_books\n",
    "#preprocess_books()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/usuaris.pkl', 'rb') as file:\n",
    "    usuaris = pickle.load(file)\n",
    "llibres = pd.read_pickle('../data/books_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " entrada_usuari (InputLayer  [(None, 53)]                 0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " entrada_llibre (InputLayer  [(None, 53)]                 0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   3456      ['entrada_usuari[0][0]']      \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   3456      ['entrada_llibre[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 128)                  0         ['dense[0][0]',               \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 256)                  33024     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  32896     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 64)                   8256      ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    65        ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 81153 (317.00 KB)\n",
      "Trainable params: 81153 (317.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dimensiones de los vectores de usuario y libro\n",
    "dim_usuari = usuaris[0].vector.shape[0]\n",
    "dim_llibre = len(llibres[\"vector\"].iloc[0])\n",
    "\n",
    "# Entradas\n",
    "entrada_usuari = Input(shape=(dim_usuari,), name='entrada_usuari')\n",
    "entrada_llibre = Input(shape=(dim_llibre,), name='entrada_llibre')\n",
    "\n",
    "# Procesamiento de las entradas\n",
    "proc_usuari = Dense(64, activation='relu')(entrada_usuari)\n",
    "proc_llibre = Dense(64, activation='relu')(entrada_llibre)\n",
    "\n",
    "# Combinación de características\n",
    "combinacio = Concatenate()([proc_usuari, proc_llibre])\n",
    "\n",
    "# Capas ocultas\n",
    "oculta1 = Dense(256, activation='relu')(combinacio)\n",
    "oculta2 = Dense(128, activation='relu')(oculta1)\n",
    "oculta3 = Dense(64, activation='relu')(oculta2)\n",
    "\n",
    "# Capa de salida\n",
    "sortida = Dense(1, activation='linear')(oculta3)\n",
    "\n",
    "# Creación del modelo\n",
    "model = Model(inputs=[entrada_usuari, entrada_llibre], outputs=sortida)\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_llibre(id_llibre):\n",
    "    return llibres[llibres[\"isbn13\"] == id_llibre][\"vector\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa listas vacías para almacenar los datos\n",
    "vectores_usuarios = []\n",
    "vectores_libros = []\n",
    "puntuaciones = []\n",
    "\n",
    "for user in usuaris:\n",
    "    # Obtiene el vector del usuario\n",
    "    vector_usuario = user.vector\n",
    "\n",
    "    # Para cada libro leído y su puntuación\n",
    "    for isbn, val in zip(user.llibres_usuari, user.val_llibres):\n",
    "        # Obtiene el vector del libro (modifica esto según tu implementación)\n",
    "        vector_libro = vector_llibre(isbn)\n",
    "\n",
    "        # Añade los datos al conjunto de datos\n",
    "        vectores_usuarios.append(vector_usuario)\n",
    "        vectores_libros.append(vector_libro)\n",
    "        puntuaciones.append(val)\n",
    "\n",
    "    # Repite para los libros recomendados\n",
    "    for isbn, punt in zip(user.llibres_recomanats, user.puntuacio_llibres):\n",
    "        # Obtiene el vector del libro (modifica esto según tu implementación)\n",
    "        vector_libro = vector_llibre(isbn)\n",
    "\n",
    "        # Añade los datos al conjunto de datos\n",
    "        vectores_usuarios.append(vector_usuario)\n",
    "        vectores_libros.append(vector_libro)\n",
    "        puntuaciones.append(punt)\n",
    "\n",
    "# Convertir las listas en arrays de NumPy para su uso con TensorFlow/Keras\n",
    "import numpy as np\n",
    "\n",
    "vectores_usuarios_np = np.array(vectores_usuarios)\n",
    "vectores_libros_np = np.array(vectores_libros)\n",
    "puntuaciones_np = np.array(puntuaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 53)\n",
      "(800, 53)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(vectores_usuarios_np.shape)\n",
    "print(vectores_libros_np.shape)\n",
    "print(puntuaciones_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np.hstack((vectores_usuarios_np, vectores_libros_np)),  # Concatenar vectores de usuario y libro\n",
    "    puntuaciones_np, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 106)\n",
      "(160, 106)\n",
      "(640,)\n",
      "(160,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 9ms/step - loss: 5.6684 - val_loss: 3.8513\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2001 - val_loss: 2.8704\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.8477 - val_loss: 2.9232\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7863 - val_loss: 2.8650\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5967 - val_loss: 2.9089\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5360 - val_loss: 3.0609\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3207 - val_loss: 3.1532\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1566 - val_loss: 3.2734\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9856 - val_loss: 3.4804\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.6801 - val_loss: 3.6041\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "historial = model.fit(\n",
    "    [X_train[:, :53], X_train[:, 53:]],  # Separar los vectores de usuario y libro\n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 955us/step - loss: 3.4482\n",
      "Pérdida en el conjunto de prueba: 3.448214054107666\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo con el conjunto de prueba\n",
    "evaluacion = model.evaluate(\n",
    "    [X_test[:, :53], X_test[:, 53:]], \n",
    "    y_test\n",
    ")\n",
    "\n",
    "print(f'Pérdida en el conjunto de prueba: {evaluacion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 978us/step\n",
      "Precisión aproximada: 16.25%\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "predicciones = model.predict([X_test[:, :53], X_test[:, 53:]])\n",
    "\n",
    "# Definir un umbral de aceptación\n",
    "umbral = 0.5\n",
    "\n",
    "# Calcular el número de predicciones \"correctas\"\n",
    "predicciones_correctas = np.abs(predicciones.flatten() - y_test) <= umbral\n",
    "\n",
    "# Calcular la precisión\n",
    "precision = np.mean(predicciones_correctas)\n",
    "print(f'Precisión aproximada: {precision * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
