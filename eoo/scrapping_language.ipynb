{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = 'data.json'\n",
    "pkl_name = 'casos.pkl'\n",
    "csv_name = 'casos.csv'\n",
    "carpeta = ''\n",
    "pkl_name_ll = 'llibres.pkl'\n",
    "csv_name_ll = 'llibres.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# np seed = 0\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if casos.pkl exists, load it\n",
    "try:\n",
    "    casos = pd.read_pickle(carpeta+pkl_name)\n",
    "    get = False\n",
    "except:\n",
    "    get = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # URL del archivo JSON comprimido\n",
    "    url = 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/goodreads_reviews_dedup.json.gz'\n",
    "\n",
    "    # Realizar la solicitud GET al servidor\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Verificar si la solicitud fue exitosa (código de estado 200)\n",
    "    if response.status_code == 200:\n",
    "        # Descomprimir el contenido del archivo\n",
    "        with gzip.GzipFile(fileobj=response.raw) as f:\n",
    "            # Leer las primeras 500 filas del JSON\n",
    "            primeras_500_filas = [json.loads(next(f)[:-1].decode('utf-8')) for _ in range(500000)]\n",
    "\n",
    "        print(\"JSON creat.\")\n",
    "    else:\n",
    "        print(f\"Error al descargar el archivo. Código de estado: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Read eoo.json only user_id, book_id, rating\n",
    "    df = pd.DataFrame(primeras_500_filas)\n",
    "    df = df[['user_id', 'book_id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Plot rating distribution and save to eoo/rating_distribution.png\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='rating', data=df)\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Rating Distribution')\n",
    "    plt.savefig(f'{carpeta}rating_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Give me unique users\n",
    "    unique_users = df['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Make a database with unique users, list of books rated and list of rating for each book\n",
    "    df_aux = pd.DataFrame(columns=['user_id', 'books', 'ratings'])\n",
    "\n",
    "    for user in unique_users:\n",
    "        # Filter by user\n",
    "        user_df = df[df['user_id'] == user]\n",
    "        # Get list of books rated by user\n",
    "        books = user_df['book_id'].tolist()\n",
    "        # Get list of ratings for each book\n",
    "        ratings = user_df['rating'].tolist()\n",
    "        # Create a dictionary with books and ratings\n",
    "        user_dict = dict(zip(books, ratings))\n",
    "        # Save user, books and ratings in df_aux using pd.concat\n",
    "        df_aux = pd.concat([df_aux, pd.DataFrame({'user_id': [user], 'books': [books], 'ratings': [ratings]})])\n",
    "\n",
    "    df_aux = df_aux.reset_index(drop=True)\n",
    "\n",
    "    print(\"Dataset joined. Unique users:\", len(df_aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Plot how many books each user has rated and save to eoo/books_rated_before.png\n",
    "    # x: each user\n",
    "    # y: number of books rated\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xlabel('user_id')\n",
    "    plt.ylabel('Number of books rated')\n",
    "    plt.title('Number of books rated by each user')\n",
    "    plt.plot(df_aux['user_id'], df_aux['books'].apply(lambda x: len(x)))\n",
    "    plt.savefig(f'{carpeta}books_rated_before.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    min_books = 10\n",
    "    max_books = 20\n",
    "\n",
    "    # Remove users that have rated less than 10 books and more than 50\n",
    "    df_aux = df_aux[df_aux['books'].apply(lambda x: len(x) >= min_books and len(x) <= max_books)]\n",
    "    df_aux = df_aux.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Dataset filtered with users with more than {min_books} and less than {max_books} books reviewed. Unique users:\", len(df_aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Plot how many books each user has rated and save to eoo/books_rated_after.png\n",
    "    # x: each user\n",
    "    # y: number of books rated\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xlabel('user_id')\n",
    "    plt.ylabel('Number of books rated')\n",
    "    plt.title('Number of books rated by each user')\n",
    "    plt.plot(df_aux['user_id'], df_aux['books'].apply(lambda x: len(x)))\n",
    "    plt.savefig(f'{carpeta}books_rated_after.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # For each user get 3 last books and their ratings and put them in a new column \"llibres_recomanata\" i \"puntuacions_llibres\". Then remove the 3 books from the list of books rated by the user.\n",
    "    df_aux['llibres_recomanats'] = df_aux['books'].apply(lambda x: x[-3:])\n",
    "    df_aux['puntuacions_llibres'] = df_aux['ratings'].apply(lambda x: x[-3:])\n",
    "    df_aux['books'] = df_aux['books'].apply(lambda x: x[:-3])\n",
    "    df_aux['ratings'] = df_aux['ratings'].apply(lambda x: x[:-3])\n",
    "\n",
    "    print(\"Done creating new columns.\")\n",
    "\n",
    "    # Change \"books\" and \"ratings\" columns to \"llibres_usuari\" and \"val_llibres\"\n",
    "    df_aux = df_aux.rename(columns={'books': 'llibres_usuari', 'ratings': 'val_llibres'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    df_aux.to_pickle(pkl_name)\n",
    "    df_aux.to_csv(csv_name, index=False)\n",
    "casos = pd.read_pickle(carpeta+pkl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    llibres = pd.read_pickle(carpeta+pkl_name_ll)\n",
    "    get = False\n",
    "except:\n",
    "    get = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # For each row, add all the books from \"llibres_usuari\" and \"llibres_recomanats\" to a set\n",
    "    set_llibres = set()\n",
    "    for index, row in casos.iterrows():\n",
    "        for llibre in row['llibres_usuari']:\n",
    "            set_llibres.add(llibre)\n",
    "        for llibre in row['llibres_recomanats']:\n",
    "            set_llibres.add(llibre)\n",
    "\n",
    "    set_llibres = list(set_llibres)\n",
    "    print(len(set_llibres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    fitxer = \"/Users/ucemarc/Downloads/goodreads_books.json\"\n",
    "    # Crear un DataFrame vacío para almacenar los libros que coincidan\n",
    "    df_llibres = pd.DataFrame(columns=['isbn', 'book_id', 'similar_books', 'average_rating', 'description', 'authors', 'isbn13', 'num_pages', 'publication_year', 'title', 'language_code', 'format', 'series'])\n",
    "\n",
    "    # Leer el archivo línea por línea\n",
    "    i = 1\n",
    "    with open(fitxer, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            book = json.loads(line)\n",
    "            if book['book_id'] in set_llibres:\n",
    "                print(i)\n",
    "                # Only keep the columns \"isbn\", \"book_id\", \"similar_books\", \"average_rating\", \"similar_books\", \"description\", \"authors\", \"isbn13\", \"num_pages\", \"publication_year\", \"title\" and \"language_code\"\n",
    "                book = {k: book[k] for k in ['isbn', 'book_id', 'similar_books', 'average_rating', 'similar_books', 'description', 'authors', 'isbn13', 'num_pages', 'publication_year', 'title', 'language_code', 'format', 'series']}\n",
    "                aut = []\n",
    "                for author in book['authors']:\n",
    "                    aut.append(author['author_id'])\n",
    "                book['authors'] = aut\n",
    "                # Convert the dictionary to a DataFrame\n",
    "                book = pd.DataFrame([book], index=[0])\n",
    "                # Add the book to the DataFrame\n",
    "                df_llibres = pd.concat([df_llibres, pd.DataFrame(book, index=[0])])\n",
    "                i += 1\n",
    "    df_llibres.to_csv(\"llibres.csv\", index=False)\n",
    "    df_llibres.to_pickle(\"llibres.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If column \"genres\" exists in llibres.pkl then get = False\n",
    "try:\n",
    "    llibres = pd.read_pickle(carpeta+pkl_name_ll)\n",
    "    llibres['genres']\n",
    "    get = False\n",
    "except:\n",
    "    get = True\n",
    "    df_llibres = pd.read_csv(carpeta+csv_name_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    fitxer = \"/Users/ucemarc/Downloads/goodreads_book_genres_initial.json\"\n",
    "\n",
    "    # Crear un DataFrame vacío para almacenar los libros que coincidan\n",
    "    df_genres = pd.DataFrame(columns=['book_id', 'genres'])\n",
    "\n",
    "    with open(fitxer, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            book = json.loads(line)\n",
    "            if book['book_id'] in set_llibres:\n",
    "                # Only keep the columns \"isbn\", \"book_id\", \"similar_books\", \"average_rating\", \"similar_books\", \"description\", \"authors\", \"isbn13\", \"num_pages\", \"publication_year\", \"title\" and \"language_code\"\n",
    "                book = {k: book[k] for k in ['book_id', 'genres']}\n",
    "                # Get only the keys of the dictionary\n",
    "                book['genres'] = list(book['genres'].keys())\n",
    "                # Convert the dictionary to a DataFrame\n",
    "                book = pd.DataFrame([book], index=[0])\n",
    "                # Add the book to the DataFrame\n",
    "                df_genres = pd.concat([df_genres, pd.DataFrame(book, index=[0])])\n",
    "                df_genres.to_csv(\"genres.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Merge df_llibres and df_genres on book_id\n",
    "    df_llibres['book_id'] = df_llibres['book_id'].astype(int)\n",
    "    df_genres['book_id'] = df_genres['book_id'].astype(int)\n",
    "    df_llibres= pd.merge(df_llibres, df_genres, on='book_id', how='inner')\n",
    "    df_llibres.to_csv(\"llibres.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Check how many unique genres there are\n",
    "    unique_genres = set()\n",
    "    for index, row in df_llibres.iterrows():\n",
    "        for genre in row['genres']:\n",
    "            unique_genres.add(genre)\n",
    "    print(len(unique_genres))\n",
    "    print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Replace 'history, historical fiction, biography' to 'history'\n",
    "    df_llibres['genres'] = df_llibres['genres'].apply(lambda x: ['history' if i == 'history, historical fiction, biography' else i for i in x])\n",
    "    # Replace 'fantasy, paranormal' to 'fantasy'\n",
    "    df_llibres['genres'] = df_llibres['genres'].apply(lambda x: ['fantasy' if i == 'fantasy, paranormal' else i for i in x])\n",
    "    # Replace 'mystery, thriller, crime' to 'mystery'\n",
    "    df_llibres['genres'] = df_llibres['genres'].apply(lambda x: ['mystery' if i == 'mystery, thriller, crime' else i for i in x])\n",
    "    # Replace 'comics, graphic' to 'comics'\n",
    "    df_llibres['genres'] = df_llibres['genres'].apply(lambda x: ['comics' if i == 'comics, graphic' else i for i in x])\n",
    "    df_llibres.to_csv(carpeta+csv_name_ll, index=False)\n",
    "    df_llibres.to_pickle(carpeta+pkl_name_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get:\n",
    "    # Check how many unique genres there are\n",
    "    unique_genres = set()\n",
    "    for index, row in df_llibres.iterrows():\n",
    "        for genre in row['genres']:\n",
    "            unique_genres.add(genre)\n",
    "    print(len(unique_genres))\n",
    "    print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llibres = pd.read_pickle(carpeta+pkl_name_ll)\n",
    "casos = pd.read_pickle(carpeta+pkl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"estil_literari\": [\"realisme\", \"romanticisme\", \"naturalisme\", \"simbolisme\", \"modernisme\", \"realisme magico\", \"postmodernisme\"],\n",
    "    \"complexitat\": [\"baixa\", \"mitjana\", \"alta\"],\n",
    "    \"caracteristiques\": [\"simples\", \"complexes\"],\n",
    "    \"desenvolupament_del_personatge\": [\"baix\", \"mitja\", \"alt\"],\n",
    "    \"accio_o_reflexio\": [\"accio\", \"reflexio\"],\n",
    "    \"epoca\": [\"actual\", \"passada\", \"futura\"],\n",
    "    \"detall_cientific\": [\"baix\", \"mitja\", \"alta\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(length1, length2, unique_min, unique_max, categorie):\n",
    "    # Número de valores únicos (entre 2 y 4)\n",
    "    num_unique_values = np.random.randint(unique_min, unique_max)\n",
    "\n",
    "    # Seleccionar valores únicos de forma aleatoria\n",
    "    unique_values = np.random.choice(categories[categorie], size=num_unique_values, replace=False)\n",
    "\n",
    "    # Crear el vector de 10 posiciones\n",
    "    vector1 = [np.random.choice(unique_values) for _ in range(length1)]\n",
    "    vector2 = [np.random.choice(unique_values) for _ in range(length2)]\n",
    "    return vector1, vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funció auxiliar per actualitzar els diccionaris\n",
    "def actualitzar_diccionaris(llibre_id, valor, diccionari):\n",
    "    if valor in diccionari[llibre_id]:\n",
    "        diccionari[llibre_id][valor] += 1\n",
    "    else:\n",
    "        diccionari[llibre_id][valor] = 1\n",
    "\n",
    "# Inicialització de diccionaris per a cada atribut\n",
    "estil_literari = [{} for _ in range(len(llibres))]\n",
    "complexitat = [{} for _ in range(len(llibres))]\n",
    "caracteristiques = [{} for _ in range(len(llibres))]\n",
    "desenvolupament_del_personatge = [{} for _ in range(len(llibres))]\n",
    "accio_o_reflexio = [{} for _ in range(len(llibres))]\n",
    "epoca = [{} for _ in range(len(llibres))]\n",
    "detall_cientific = [{} for _ in range(len(llibres))]\n",
    "\n",
    "for index, row in casos.iterrows():\n",
    "    len_llibres_usuari = len(row['llibres_usuari'])\n",
    "    len_llibres_recomanats = len(row['llibres_recomanats'])\n",
    "    estil_literari1, estil_literari2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 2, 4, \"estil_literari\")\n",
    "    complexitat1, complexitat2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 1, 3, \"complexitat\")\n",
    "    caracteristiques1, caracteristiques2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 1, 3, \"caracteristiques\")\n",
    "    desenvolupament_del_personatge1, desenvolupament_del_personatge2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 1, 3, \"desenvolupament_del_personatge\")\n",
    "    accio_o_reflexio1, accio_o_reflexio2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 1, 2, \"accio_o_reflexio\")\n",
    "    epoca1, epoca2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 1, 3, \"epoca\")\n",
    "    detall_cientific1, detall_cientific2 = make_vector(len_llibres_usuari, len_llibres_recomanats, 1, 3, \"detall_cientific\")\n",
    "\n",
    "    for i in range(len_llibres_usuari):\n",
    "        llibre_id_usuari = llibres[llibres[\"book_id\"] == int(row['llibres_usuari'][i])].index[0]\n",
    "        actualitzar_diccionaris(llibre_id_usuari, estil_literari1[i], estil_literari)\n",
    "        actualitzar_diccionaris(llibre_id_usuari, complexitat1[i], complexitat)\n",
    "        actualitzar_diccionaris(llibre_id_usuari, caracteristiques1[i], caracteristiques)\n",
    "        actualitzar_diccionaris(llibre_id_usuari, desenvolupament_del_personatge1[i], desenvolupament_del_personatge)\n",
    "        actualitzar_diccionaris(llibre_id_usuari, accio_o_reflexio1[i], accio_o_reflexio)\n",
    "        actualitzar_diccionaris(llibre_id_usuari, epoca1[i], epoca)\n",
    "        actualitzar_diccionaris(llibre_id_usuari, detall_cientific1[i], detall_cientific)\n",
    "\n",
    "    for i in range(len_llibres_recomanats):\n",
    "        llibre_id_recomanat = llibres[llibres[\"book_id\"] == int(row['llibres_recomanats'][i])].index[0]\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, estil_literari2[i], estil_literari)\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, complexitat2[i], complexitat)\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, caracteristiques2[i], caracteristiques)\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, desenvolupament_del_personatge2[i], desenvolupament_del_personatge)\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, accio_o_reflexio2[i], accio_o_reflexio)\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, epoca2[i], epoca)\n",
    "        actualitzar_diccionaris(llibre_id_recomanat, detall_cientific2[i], detall_cientific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the most voted value for each book\n",
    "for i in range(len(llibres)):\n",
    "    if len(estil_literari[i]) > 0:\n",
    "        estil_literari[i] = max(estil_literari[i], key=estil_literari[i].get)\n",
    "    if len(complexitat[i]) > 0:\n",
    "        complexitat[i] = max(complexitat[i], key=complexitat[i].get)\n",
    "    if len(caracteristiques[i]) > 0:\n",
    "        caracteristiques[i] = max(caracteristiques[i], key=caracteristiques[i].get)\n",
    "    if len(desenvolupament_del_personatge[i]) > 0:\n",
    "        desenvolupament_del_personatge[i] = max(desenvolupament_del_personatge[i], key=desenvolupament_del_personatge[i].get)\n",
    "    if len(accio_o_reflexio[i]) > 0:\n",
    "        accio_o_reflexio[i] = max(accio_o_reflexio[i], key=accio_o_reflexio[i].get)\n",
    "    if len(epoca[i]) > 0:\n",
    "        epoca[i] = max(epoca[i], key=epoca[i].get)\n",
    "    if len(detall_cientific[i]) > 0:\n",
    "        detall_cientific[i] = max(detall_cientific[i], key=detall_cientific[i].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15666\n",
      "15666\n"
     ]
    }
   ],
   "source": [
    "print(len(estil_literari))\n",
    "print(len(llibres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afegir les noves columnes al DataFrame\n",
    "llibres[\"estil_literari\"] = estil_literari\n",
    "llibres[\"complexitat\"] = complexitat\n",
    "llibres[\"caracteristiques\"] = caracteristiques\n",
    "llibres[\"desenvolupament_del_personatge\"] = desenvolupament_del_personatge\n",
    "llibres[\"accio_o_reflexio\"] = accio_o_reflexio\n",
    "llibres[\"epoca\"] = epoca\n",
    "llibres[\"detall_cientific\"] = detall_cientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llibres.to_pickle(pkl_name_ll)\n",
    "llibres.to_csv(csv_name_ll, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓ PER ELIMINAR DE SIMILARS AQUELLS LLIBRES QUE NO ESTAN A LA BASE DE DADES\n",
    "# Carregar les dades del CSV\n",
    "df = pd.read_csv(csv_name_ll)\n",
    "\n",
    "# Funció per convertir la cadena de la llista en una llista real i netejar-la\n",
    "def neteja_similars(similars, ids_valids):\n",
    "    # Convertir la cadena a una llista\n",
    "    similars_list = similars.strip(\"[]\").replace(\"'\", \"\").split(\", \")\n",
    "    # Mantenir només els IDs que estan presents en ids_valids\n",
    "    return [id for id in similars_list if id in ids_valids]\n",
    "\n",
    "# Obtenir els book_id com a conjunt per a una cerca més ràpida\n",
    "ids_valids = set(df['book_id'].astype(str))\n",
    "df['similar_books'] = df['similar_books'].apply(lambda x: neteja_similars(x, ids_valids))\n",
    "\n",
    "# Funció per assignar 'noisbn'\n",
    "def assigna_noisbn(valor):\n",
    "    if pd.isna(valor) or valor == 'NaN':\n",
    "        return 'noisbn'\n",
    "    else:\n",
    "        return valor\n",
    "\n",
    "# Aplicar la funció a les columnes isbn i isbn13\n",
    "df['isbn'] = df['isbn'].apply(assigna_noisbn)\n",
    "df['isbn13'] = df['isbn13'].apply(assigna_noisbn)\n",
    "\n",
    "\n",
    "# Funció per assignar 'no_identificat' a language_code\n",
    "def assigna_noidentificat(valor):\n",
    "    if valor == 'eng' or valor == 'en-US' or valor == 'en-GB' or valor == 'en-CA' or valor == 'en':\n",
    "        return 'en'\n",
    "    if pd.isna(valor) or valor == 'NaN' or valor =='--':\n",
    "        return 'no_identificat'\n",
    "    else:\n",
    "        return valor\n",
    "\n",
    "# Aplicar la funció a les columnes isbn i isbn13\n",
    "df['language_code'] = df['language_code'].apply(assigna_noidentificat)\n",
    "\n",
    "df.to_csv(csv_name_ll, index=False)\n",
    "df.to_pickle(pkl_name_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ebook' 'tapa blanda' 'audio' 'tapa dura']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(pkl_name_ll)\n",
    "# imprimir les diferents categories de \"format\"\n",
    "print(df['format'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificar les diferents categories de audio\n",
    "df['format'] = df['format'].apply(lambda x: 'audio' if x in ['Audible Audio', 'Audio CD', 'Audio Cassette', 'Audio', 'Audiobook', 'audio cd', 'MP3 CD'] else x)\n",
    "# Unificar les diferents categories de ebook\n",
    "df['format'] = df['format'].apply(lambda x: 'ebook' if x in ['ebook', 'Kindle Edition', 'HTML', 'Kindle', 'chapbook/ebook', 'Serialized Digital Download'] else x)\n",
    "# Unificar les diferents categories de paper\n",
    "df['format'] = df['format'].apply(lambda x: 'tapa blanda' if x in ['Paperback', 'Mass Market Paperback', 'paper', 'Trade Paperback', 'pocket', 'Softcover', 'Trade paperback', 'Paper Back', 'Perfect Paperback', 'paperback', 'Trade Paper', 'Paberback', 'Softcover with Flap', 'Tapa blanda con solapas'] else x)\n",
    "# Unificar les diferents categories de hardcover\n",
    "df['format'] = df['format'].apply(lambda x: 'tapa dura' if x in ['Hardcover', 'Board book', 'Board Book', 'Hardback', 'hardcover', 'issue', 'Broche', 'Klappenbroschur', 'Nook', 'Library Binding', 'Gebunden', 'Wen Ku', 'Leather Bound', 'Musc. Supplies', 'Podiobook', 'Brossura', 'Nook Book', 'Spiral-bound', 'Novelty Book', 'Glf `dy,', 'Misc. Supplies', 'Broschiert', 'Unknown Binding'] else x)\n",
    "# Unificar les diferents categories de comic\n",
    "df['format'] = df['format'].apply(lambda x: 'tapa blanda' if x in ['comics', 'Thirteen interactive chapters.', 'Graphic Novel', 'Digital comic', 'Comic Book'] else x)\n",
    "# Poner los nan a las otras categorias siguiendo la distribución de las categorias actuales\n",
    "df['format'] = df['format'].apply(lambda x: np.random.choice(['tapa blanda', 'tapa dura', 'ebook']) if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ebook' 'tapa blanda' 'audio' 'tapa dura']\n",
      "format\n",
      "tapa blanda    7802\n",
      "tapa dura      4916\n",
      "ebook          2799\n",
      "audio           149\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# imprimir les diferents categories de \"format\"\n",
    "print(df['format'].unique())\n",
    "# Print how many books there are for each format\n",
    "print(df['format'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_name_ll, index=False)\n",
    "df.to_pickle(pkl_name_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos los valores menores a 1.5 en average_rating a la media\n",
    "df['average_rating'] = df['average_rating'].apply(lambda x: np.random.uniform(1.5, 5) if x < 1.5 else x)\n",
    "df.to_csv(csv_name_ll, index=False)\n",
    "df.to_pickle(pkl_name_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pages\n",
      "NaN       2135\n",
      "320.0      300\n",
      "288.0      256\n",
      "304.0      245\n",
      "336.0      241\n",
      "          ... \n",
      "677.0        1\n",
      "615.0        1\n",
      "2201.0       1\n",
      "3164.0       1\n",
      "649.0        1\n",
      "Name: count, Length: 941, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check value count for num_pages including nan\n",
    "print(df['num_pages'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISBN: 9780800759490 - Pages: 207\n",
      "ISBN: 9781632150066 - No info\n",
      "ISBN: 9780316021555 - Pages: 448\n",
      "ISBN: 9780765312273 - No info\n",
      "ISBN: 9781743580158 - No info 2\n",
      "ISBN: 9783442723430 - Pages: 636\n",
      "ISBN: 9780312331467 - Pages: 308\n",
      "ISBN: 9780345451323 - Pages: 340\n",
      "ISBN: 9781585679119 - Pages: 206\n",
      "ISBN: 9780826328090 - Pages: 228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m isbn \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39misbn13\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m isbn \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoisbn\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     info_libro \u001b[39m=\u001b[39m obtener_info_libro(isbn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m info_libro \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNo se encontró información para ese ISBN.\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbibkeys\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mISBN:\u001b[39m\u001b[39m{\u001b[39;00misbn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mjscmd\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(base_url, params\u001b[39m=\u001b[39;49mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliviapc/Documents/GitHub/SBC_practica/eoo/scrapping_language.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mISBN:\u001b[39m\u001b[39m{\u001b[39;00misbn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m data:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1096\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1099\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1100\u001b[0m         (\n\u001b[1;32m   1101\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mconn\u001b[39m.\u001b[39mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m is_time_off:\n\u001b[1;32m    634\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    635\u001b[0m         (\n\u001b[1;32m    636\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSystem time is way off (before \u001b[39m\u001b[39m{\u001b[39;00mRECENT_DATE\u001b[39m}\u001b[39;00m\u001b[39m). This will probably \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[1;32m    640\u001b[0m     )\n\u001b[0;32m--> 642\u001b[0m sock_and_verified \u001b[39m=\u001b[39m _ssl_wrap_socket_and_match_hostname(\n\u001b[1;32m    643\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    644\u001b[0m     cert_reqs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_reqs,\n\u001b[1;32m    645\u001b[0m     ssl_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_version,\n\u001b[1;32m    646\u001b[0m     ssl_minimum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_minimum_version,\n\u001b[1;32m    647\u001b[0m     ssl_maximum_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_maximum_version,\n\u001b[1;32m    648\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    649\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    650\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    651\u001b[0m     cert_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    652\u001b[0m     key_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    653\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    654\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    655\u001b[0m     ssl_context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl_context,\n\u001b[1;32m    656\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    657\u001b[0m     assert_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_hostname,\n\u001b[1;32m    658\u001b[0m     assert_fingerprint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_fingerprint,\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    660\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39msocket\n\u001b[1;32m    661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_verified \u001b[39m=\u001b[39m sock_and_verified\u001b[39m.\u001b[39mis_verified\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:782\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    780\u001b[0m         server_hostname \u001b[39m=\u001b[39m normalized\n\u001b[0;32m--> 782\u001b[0m ssl_sock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    783\u001b[0m     sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    784\u001b[0m     keyfile\u001b[39m=\u001b[39;49mkey_file,\n\u001b[1;32m    785\u001b[0m     certfile\u001b[39m=\u001b[39;49mcert_file,\n\u001b[1;32m    786\u001b[0m     key_password\u001b[39m=\u001b[39;49mkey_password,\n\u001b[1;32m    787\u001b[0m     ca_certs\u001b[39m=\u001b[39;49mca_certs,\n\u001b[1;32m    788\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49mca_cert_dir,\n\u001b[1;32m    789\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49mca_cert_data,\n\u001b[1;32m    790\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    791\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    792\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    793\u001b[0m )\n\u001b[1;32m    795\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[39mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/util/ssl_.py:470\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:  \u001b[39m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n\u001b[1;32m    471\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/util/ssl_.py:514\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    511\u001b[0m     SSLTransport\u001b[39m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 514\u001b[0m \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    456\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    457\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    458\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    459\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    460\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    461\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    462\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1046\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1044\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1047\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1048\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1317\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1318\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def obtener_info_libro(isbn):\n",
    "    base_url = \"https://openlibrary.org/api/books\"\n",
    "    params = {\n",
    "        \"bibkeys\": f\"ISBN:{isbn}\",\n",
    "        \"format\": \"json\",\n",
    "        \"jscmd\": \"data\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if f\"ISBN:{isbn}\" in data:\n",
    "            book_info = data[f\"ISBN:{isbn}\"]\n",
    "            return book_info  # Devuelve todos los campos disponibles\n",
    "        else:\n",
    "            return \"No se encontró información para ese ISBN.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Para los libros que no tienen num_pages, obtener el número de páginas de OpenLibrary\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['num_pages']):\n",
    "        isbn = row['isbn13']\n",
    "        if isbn != 'noisbn':\n",
    "            info_libro = obtener_info_libro(isbn)\n",
    "            if info_libro != \"No se encontró información para ese ISBN.\":\n",
    "                try:\n",
    "                    num_pages = info_libro['number_of_pages']\n",
    "                    df.at[index, 'num_pages'] = num_pages\n",
    "                    print(f\"ISBN: {isbn} - Pages: {num_pages}\")\n",
    "                except:\n",
    "                    print(f\"ISBN: {isbn} - No info\")\n",
    "            else:\n",
    "                print(f\"ISBN: {isbn} - No info 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def obtener_numero_de_paginas(isbn):\n",
    "    # Construir la URL para la consulta a la API\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}\"\n",
    "\n",
    "    # Realizar la solicitud\n",
    "    respuesta = requests.get(url)\n",
    "\n",
    "    # Verificar si la solicitud fue exitosa\n",
    "    if respuesta.status_code == 200:\n",
    "        datos = respuesta.json()\n",
    "\n",
    "        # Obtener la información del libro\n",
    "        if \"items\" in datos:\n",
    "            libro = datos[\"items\"][0]\n",
    "            if \"pageCount\" in libro[\"volumeInfo\"]:\n",
    "                return libro[\"volumeInfo\"][\"pageCount\"]\n",
    "            else:\n",
    "                return \"Número de páginas no disponible\"\n",
    "        else:\n",
    "            return \"ISBN no encontrado\"\n",
    "    else:\n",
    "        return \"Error en la solicitud\"\n",
    "\n",
    "# Para los libros que no tienen num_pages, obtener el número de páginas de OpenLibrary\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['num_pages']):\n",
    "        isbn = row['isbn13']\n",
    "        if isbn != 'noisbn':\n",
    "            num_pag = obtener_numero_de_paginas(isbn)\n",
    "            if num_pag != \"Error en la solicitud\" or num_pag != \"ISBN no encontrado\" or num_pag != \"Número de páginas no disponible\":\n",
    "                df.at[index, 'num_pages'] = num_pag\n",
    "                print(f\"ISBN: {isbn} - Pages: {num_pag}\")\n",
    "            else:\n",
    "                print(f\"ISBN: {isbn} - No info 2\")\n",
    "\n",
    "\n",
    "\n",
    "# Lista de ISBNs\n",
    "isbns = [\"9780321534965\", \"9780321563842\", \"otro_isbn\"]\n",
    "\n",
    "# Tu API Key\n",
    "api_key = \"TU_API_KEY\"\n",
    "\n",
    "# Consultar cada ISBN y obtener el número de páginas\n",
    "for isbn in isbns:\n",
    "    print(f\"ISBN: {isbn} - Número de páginas: {obtener_numero_de_paginas(isbn, api_key)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pages\n",
      "NaN       1375\n",
      "320.0      323\n",
      "288.0      274\n",
      "304.0      266\n",
      "336.0      256\n",
      "          ... \n",
      "706.0        1\n",
      "1248.0       1\n",
      "4081.0       1\n",
      "1116.0       1\n",
      "649.0        1\n",
      "Name: count, Length: 952, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check value count for num_pages including nan\n",
    "print(df['num_pages'].value_counts(dropna=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
